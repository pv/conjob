#!/usr/bin/env python3
# -*-python-*-
"""
conjob [options] COMMAND [...]

Process job controller, letting a limited number of jobs run at the same
time.

Commands:
  list                List scheduled PIDs
  schedule [COUNT]    Perfom scheduling, starting/stopping processes
  setprio JIDS PRIO   Set a priority for a PID
  run CMD             Run a command and put the process in the queue
  bg CMD              Run a command in the background and insert to queue
                      Redirect the output to a file in ~/.conjob.log/
  bg_many < CMDS      Queue many commands at once; one per line
  kill JIDS           Kill jobs
  tail JIDS           Tail -f logfiles
  less JIDS           Page through logfiles
  stats JIDS          Display job statistics
  cleanup JIDS        Remove completed jobs from the job list
  requeue JIDS        Kill and re-queue given jobs
  dump_jobs [FILE]    Dump a job listing to FILE or stdout
  load_jobs [FILE]    Load a job listing from FILE or stdin

  'JIDS' can be a comma separated list of JIDs, or a shell glob pattern.
  The syntax '{MIN-MAX}' matches all integers from MIN to MAX.
  Prepending '/' to the pattern makes it match also completed jobs,
  '@' the command lines, '%' the PIDs.

"""
import os, sys, time, optparse, signal, tempfile, shutil, re, fnmatch
import fcntl
import json
import datetime, subprocess, atexit
import configparser
import pickle

CFGFILE = os.path.expanduser("~/.conjob")

class Job(object):
    """
    Representation of a pending or running job.

    """
    date_format = '%Y-%m-%d %H:%M:%S'

    def __init__(self, jid, cmd, log_file):
        self.jid = jid
        self.process = None
        self.cmd = cmd
        self.cwd = os.getcwd()
        self.log_file = log_file
        self.end_time = None
        self.queue_time = datetime.datetime.now()
        self.start_time = None
        self.priority = 0.

    @classmethod
    def _strptime(cls, s):
        return datetime.datetime.strptime(s, cls.date_format)

    def update(self):
        self._update_alive_state()

    @classmethod
    def from_section(cls, section):
        self = Job(None, None, None)
        self.jid = str(section['jid'])
        pid = section.get('pid', None)
        pgid = section.get('pgid', None)
        if pid is not None:
            if pgid is not None:
                pgid = int(pgid)
            self.process = Process(int(pid), pgid)
        self.cmd = section['cmd']
        self.cwd = section.get('cwd', None)
        self.log_file = section['log_file']
        if 'end_time' in section:
            self.end_time = cls._strptime(section['end_time'])
        if 'start_time' in section:
            self.start_time = cls._strptime(section['start_time'])
        self.queue_time = cls._strptime(section['queue_time'])
        self.priority = float(section.get('priority', 0.))
        self._update_alive_state()
        return self

    def as_section(self):
        section = {}
        section['jid'] = self.jid
        if self.process is not None and self.process.is_alive:
            section['pid'] = self.process.pid
            pgid = getattr(self.process, 'pgid', None)
            if pgid is not None:
                section['pgid'] = pgid
        if self.start_time is not None:
            section['start_time'] = self.start_time
        section['cmd'] = self.cmd
        section['cwd'] = self.cwd
        section['log_file'] = self.log_file
        if self.end_time:
            section['end_time'] = self.end_time.strftime(self.date_format)
        if self.start_time:
            section['start_time'] = self.start_time.strftime(self.date_format)
        section['queue_time'] = self.queue_time.strftime(self.date_format)
        section['priority'] = self.priority
        return section

    def kill(self, signal=None):
        if self.is_alive:
            self.process.kill(signal)
            self.process.resume()
        if self.end_time is None:
            self.end_time = datetime.datetime.now()

    def reset(self):
        self.start_time = None
        self.end_time = None

        failfile = self.log_file + '.fail'
        if os.path.isfile(failfile):
            os.unlink(failfile)

    def resume(self):
        if self.is_alive:
            self.process.resume()

    def suspend(self):
        if self.is_running:
            self.process.suspend()

    def set_cpu_affinity(self, cpus):
        if self.is_alive:
            self.process.set_cpu_affinity(cpus)

    def run(self, nice=None, ioprio=None):
        if self.completed:
            return
        if self.process:
            if not self.process.is_running and self.is_alive:
                self.process.resume()
            return

        self.start_time = datetime.datetime.now()

        self.process = Process.fork_to_background(self.cmd, self.log_file,
                                                  cwd=self.cwd,
                                                  ioprio=ioprio, nice=nice,
                                                  msg="Job %s" % self.jid)

    @property
    def is_alive(self):
        self._update_alive_state()
        return (self.process is not None)

    def _update_alive_state(self):
        if self.process and not self.process.is_alive:
            self.process = None

    @property
    def is_running(self):
        if self.process is None:
            return False
        return self.process.is_running

    @property
    def completed(self):
        return (self.end_time is not None)

    @property
    def started(self):
        return (self.start_time is not None)

    @property
    def pid(self):
        if not self.process:
            return None
        return self.process.pid

    @property
    def failed(self):
        return os.path.isfile(self.log_file + '.fail')

    def __repr__(self):
        return "<Job %s: %s>" % (self.jid, shell_join(self.cmd))


class PidScheduler(object):
    def __init__(self, filename):
        self.jobs = []

        defaults = {
            'logdir': filename + '.log',
            'joblist': filename + '.jobs',
            'nprocs': 2,
            'nice': 10,
            'ioprio': 7,
            'loadavg': 2.0,
            'max_mem_percentage': 75,
            'pager': '',
            'min_vm_free': 1e9,
        }

        cfg = configparser.RawConfigParser(defaults=defaults)
        with open(filename, 'r') as f:
            text = "[options]\n" + f.read()
        cfg.read_string(text)

        cvt = {str: cfg.get, int: cfg.getint, float: cfg.getfloat}
        self.cfg = {}
        for key, value in defaults.items():
            self.cfg[key] = cvt[type(value)]('options', key)

        self.lockfile = open(CFGFILE + ".lock", "ab")
        self.lock_count = 0
        self.load_jobs()

        if not os.path.isdir(self.cfg['logdir']):
            os.makedirs(self.cfg['logdir'])

    def lock(self):
        fcntl.flock(self.lockfile.fileno(), fcntl.LOCK_EX)
        self.lock_count += 1

    def unlock(self):
        if self.lock_count <= 1:
            fcntl.flock(self.lockfile.fileno(), fcntl.LOCK_UN)
            self.lock_count = 0
        else:
            self.lock_count -= 1

    def load_jobs_cfg(self, stream):
        if isinstance(stream, str):
            with open(stream, 'r') as f:
                return self.load_jobs_cfg(f)

        sections = json.load(stream)
        for key, value in sections.items():
            self.jobs.append(Job.from_section(value))

    def dump_jobs_cfg(self, stream):
        if isinstance(stream, str):
            with open(stream, 'w') as f:
                return self.dump_jobs_cfg(f)

        sections = {}
        for job in self.jobs:
            sections[job.jid] = job.as_section()
        json.dump(sections, stream, indent=' '*2)

    def load_jobs(self):
        self.lock()
        try:
            fn = self.cfg['joblist']
            if os.path.exists(fn):
                f = open(fn, 'rb')
                try:
                    self.jobs = pickle.load(f)
                finally:
                    f.close()
            for job in self.jobs:
                job.update()
        finally:
            self.unlock()

    def save_jobs(self):
        # avoid write races by writing to a temp file and overwriting
        # by moving (which probably is atomic within a FS)
        self.lock()
        try:
            fd, tmp_fn = tempfile.mkstemp(
                prefix=os.path.basename(self.cfg['joblist']) + '-',
                dir=os.path.dirname(self.cfg['joblist']))
            os.close(fd)

            f = open(tmp_fn, 'wb')
            try:
                pickle.dump(self.jobs, f)
            finally:
                f.close()

            shutil.move(tmp_fn, self.cfg['joblist'])
        finally:
            self.unlock()

    def get_free_jid(self, prefix=""):
        if prefix:
            prefix = prefix + "-"
        else:
            prefix = ""
        def int_prefix(s):
            if not prefix:
                try: return int(s)
                except ValueError: return None
            else:
                if not s.startswith(prefix): return None
                try: return int(s[len(prefix):])
                except ValueError: return None
        imax = max([int_prefix(job.jid) for job in self.jobs
                    if int_prefix(job.jid) is not None] + [0])
        return "%s%d" % (prefix, imax+1)

    def get_logfile_name(self, cmd, jid, timestamp=None):
        if timestamp is None:
            timestamp = time.strftime('%Y-%m-%d_%H-%M-%S')
        logfile_fn = os.path.join(self.cfg['logdir'],
                                  '%s.%s.%s' % (cmd[0], jid, timestamp))
        return logfile_fn

    def save(self):
        self.save_jobs()

    def add(self, cmd, prefix=None, priority=0.):
        jid = self.get_free_jid(prefix)
        logfile = self.get_logfile_name(cmd, jid)
        job = Job(jid, cmd, logfile)
        job.priority = priority
        self.jobs.append(job)
        return job

    def add_fg_and_save(self, cmd, prefix=None, priority=0.):
        job = self.add(cmd, prefix=prefix, priority=priority)
        job.process = Process(os.getpid())
        job.start_time = datetime.datetime.now()
        job.set_cpu_affinity(list(range(os.cpu_count())))
        self.save_jobs()
        Process.exec_cmd(cmd,
                         nice=self.cfg['nice'],
                         ioprio=self.cfg['ioprio'],
                         stop=True,
                         msg="Job %s" % job.jid)

    def remove(self, job):
        job.kill()
        self.jobs.remove(job)

    def _update(self):
        to_remove = []
        for job in self.jobs:
            if job.started and not job.is_alive and not job.completed:
                # the job has finished
                job.end_time = datetime.datetime.now()
            if job.completed and not job.started:
                # the job was never run, probably killed
                to_remove.append(job)
        for job in to_remove:
            self.jobs.remove(job)

    @staticmethod
    def cmp_prio(a, b):
        # highest priority first
        v = cmp(b.priority, a.priority)
        if v != 0:
            return v
        # then, alive processes first
        v = cmp(b.is_alive, a.is_alive)
        return v

    def schedule(self):
        self.lock()
        try:
            self._schedule()
        finally:
            self.unlock()

    def _schedule(self):
        # Check status
        self._update()

        # Sort
        queue = [x for x in self.jobs if not x.completed]
        queue.sort(PidScheduler.cmp_prio)
        nprocs = self.cfg['nprocs']

        # Adjust according to load average
        running = len([z for z in queue if z.is_running])
        total_running = min(Process.get_loadavg(), Process.get_num_running()-1)
        max_nprocs = running + int(round(self.cfg['loadavg'] - total_running))
        max_nprocs = max(1, max_nprocs)
        nprocs = min(nprocs, max_nprocs)

        # Adjust according to memory usage
        mem_factor = self.cfg['max_mem_percentage'] / 100.

        total_mem = Process.get_total_mem()
        mem_max = total_mem * mem_factor
        for j, job in enumerate(queue[:nprocs]):
            if job.is_alive:
                mem_max -= job.process.vm_peak
            else:
                mem_max -= 0.05 * total_mem
            if mem_max < 0 and j > 0:
                nprocs = j
                break

        # Avoid virtual memory exhaustion
        vm_free = Process.get_free_mem() + Process.get_free_swap()
        vm_total = Process.get_total_mem() + Process.get_total_swap()
        vm_min = min(self.cfg['min_vm_free']/1024., (1 - mem_factor)*vm_total)
        vm_exhausted = (vm_free < vm_min)

        # Stop / continue
        to_run = queue[:nprocs]
        to_stop = queue[nprocs:]

        resumed = False
        skipped = 0

        for cpu, job in enumerate(to_run):
            if vm_exhausted and not job.is_alive:
                # don't start new jobs if VM is exhausted
                skipped += 1
                continue

            if not job.is_running and not resumed:
                # only resume one job per schedule cycle.
                job.run(nice=self.cfg['nice'], ioprio=self.cfg['ioprio'])
                resumed = True
            job.set_cpu_affinity(list(range(os.cpu_count())))

        for job in to_stop:
            if vm_exhausted and skipped > 0 and job.is_alive and not resumed:
                # under VM exhaustion, run any existing processes to completion
                job.resume()
                skipped -= 1
                resumed = True
                continue
            job.suspend()

        self._update()

    def jobs_iter(self, listspec, target=None):
        # -- recurse
        if not isinstance(listspec, str):
            # union
            items = set()
            for item in listspec:
                items = items.union(set(self.jobs_iter(item, target)))
            for item in self.jobs:
                if item in items:
                    yield item
            return
        elif ',' in listspec:
            # intersection
            items = None
            for spec in listspec.split(','):
                item = self.jobs_iter(spec, target)
                if items is None:
                    items = set(item)
                else:
                    items = items.intersection(set(item))
            for item in self.jobs:
                if item in items:
                    yield item
            return

        # -- filtering and match targets

        if listspec.startswith('@'):
            # match command
            listspec = listspec[1:]
            if not listspec:
                listspec = 'all'
            def target(job):
                return shell_join(job.cmd)

        if listspec.startswith('%'):
            # match pid
            listspec = listspec[1:]
            if not listspec:
                listspec = 'all'
            def target(job):
                if not job.is_alive:
                    return None
                else:
                    return str(job.process.pid)

        if target is None:
            def target(job):
                return job.jid

        if listspec.startswith('/'):
            # completed jobs only
            listspec = listspec[1:]
            if not listspec:
                listspec = 'all'
            def target(job, old_target=target):
                if not job.completed:
                    return None
                else:
                    return old_target(job)
        else:
            # no completed jobs
            def target(job, old_target=target):
                if job.completed:
                    return None
                else:
                    return old_target(job)

        if listspec.startswith('-'):
            # failed jobs only
            listspec = listspec[1:]
            if not listspec:
                listspec = 'all'
            def target(job, old_target=target):
                if not job.failed:
                    return None
                else:
                    return old_target(job)

        # -- matching

        if listspec == 'all':
            listspec = '*'

        if listspec.startswith('in:'):
            fn = listspec[3:]
            if not os.path.isfile(fn):
                print("%s is not a file" % fn)
                raise SystemExit(1)

            f = open(fn, 'r')
            try:
                lines = f.read().splitlines()
            finally:
                f.close()
            lines = [x.strip() for x in lines if x.strip()]

            patterns = [fnmatch.translate(p).replace('$', '') for p in lines]
            pat = re.compile("|".join(patterns))

            for job in self.jobs:
                t = target(job)
                if t is not None and pat.match(t):
                    yield job
        elif '{' in listspec:
            pat = re.compile(r'{(\d*)-(\d*)}')
            ms = list(pat.finditer(listspec))
            if not ms:
                return

            def toint(x, default=None):
                if not x: return default
                return int(x)

            limits = [(toint(m.group(1), -sys.maxsize),
                       toint(m.group(2), sys.maxsize)) for m in ms]

            offset = 0
            spec2 = listspec
            for k, m in enumerate(ms):
                rep = '<<INDEX>>'
                spec2 = spec2[:offset+m.start()] + rep + spec2[offset+m.end():]
                offset += len(rep) - (m.end() - m.start())

            pat = "^" + fnmatch.translate(spec2).replace('.*', '.*?')
            pat = pat.replace(r'\<\<INDEX\>\>', '(\d+)')
            pat = re.compile(pat)

            for job in self.jobs:
                t = target(job)
                if job.jid == listspec:
                    yield job
                    continue
                if t is None:
                    continue
                jm = pat.match(t)
                if jm:
                    ints = map(int, jm.groups())
                    ok = True
                    for k, (min, max) in zip(ints, limits):
                        if not min <= k <= max:
                            ok = False
                            break
                    if ok:
                        yield job
        else:
            for job in self.jobs:
                t = target(job)
                if t is not None and fnmatch.fnmatch(t, listspec) \
                       or job.jid == listspec:
                    yield job

    def redirect_to_pager(self):
        pager = self.cfg['pager']
        if not pager:
            return
        if not os.isatty(sys.stdin.fileno()):
            return
        try:
            p = subprocess.Popen(pager.strip().split(), stdin=subprocess.PIPE, encoding='utf-8')
            sys.stdout = p.stdin
            atexit.register(lambda: p.communicate())
        except OSError:
            pass

def sched_locked(func):
    def wrapper(sched, p):
        sched.lock()
        try:
            return func(sched, p)
        finally:
            sched.unlock()
    return wrapper

@sched_locked
def do_schedule(sched, p):
    if len(p.args) == 0:
        count = 1
    elif len(p.args) == 1:
        try:
            count = int(p.args[0])
            if count <= 0:
                raise ValueError()
        except ValueError:
            p.error("invalid schedule count")
    else:
        p.error("invalid number of arguments")

    for k in range(count):
        sched.schedule()
    sched.save_jobs()

@sched_locked
def do_setprio(sched, p):
    try:
        prio = p.args.pop(-1)
        prio = float(prio)
    except ValueError:
        p.error("Invalid priority")

    for job in sched.jobs_iter(p.args):
        job.priority = prio

    sched.save_jobs()

def _get_list_jobs(sched, p):
    p2 = get_sub_optparse(p)
    p2.add_option("-s", "--sort", action="store", type="choice", dest="sort",
                  choices=['prio', 'end', 'none'], default="none",
                  help="sort order [one of: 'none', 'prio', 'end']")
    p2.add_option("-r", "--running", action="store_true", dest="running_only",
                  help="show only running processes")
    p2.add_option("-a", "--alive", action="store_true", dest="alive_only",
                  help="show only alive processes")
    (options, args) = p2.parse_args(p.args)
 
    if not args:
        args = ['all']

    jobs = list(sched.jobs_iter(args))

    if options.sort == 'prio':
        jobs.sort(PidScheduler.cmp_prio)
    elif options.sort == 'end':
        jobs.sort(key=lambda job: job.end_time)

    return [job for job in jobs
            if not (options.running_only and not job.is_running)
            and not (options.alive_only and not job.is_alive)]

def do_list(sched, p):
    jobs = _get_list_jobs(sched, p)

    sched.redirect_to_pager()

    # format data
    fields = ['jid', 'pid', 'end_time', 'prio', 'cmd']
    formats = dict(jid='%-Ns', prio='%Ns', cmd='%-Ns', pid='%Ns',
                   end_time='%Ns')

    rows = []
    for job in jobs:
        pid = ""
        if job.is_running:
            mark = "*"
            pid = "%d" % job.process.pid
        elif job.is_alive:
            mark = "-"
            pid = "%d" % job.process.pid
        elif job.started and not job.completed and not job.is_alive:
            mark = "/"
        elif job.failed:
            mark = " F"
        else:
            mark = " "

        if job.completed:
            end_time = job.end_time.strftime('%Y-%m-%d %H:%M')
        else:
            end_time = ""
        
        cmdline = shell_join(job.cmd)
        rows.append(dict(jid=job.jid + mark,
                         pid=pid,
                         end_time=end_time,
                         prio="%.2g" % job.priority,
                         cmd=cmdline))

    if not rows:
        # nothing to show
        return

    # print in columns
    rowlens = dict((fld, max(len(r[fld]) for r in rows))
                   for fld in fields)
    len_formats = dict((fld, formats[fld].replace('N', str(rowlens[fld])))
                       for fld in fields)

    try:
        for row in rows:
            print(" ".join(len_formats[fld] % row[fld] for fld in fields))
    except BrokenPipeError:
        pass

def do_info(sched, p):
    jobs = _get_list_jobs(sched, p)
    
    sched.redirect_to_pager()
 
    for job in jobs:
        info = dict(
            jid=job.jid,
            cmd=shell_join(job.cmd),
            priority=job.priority,
            queue_time=job.queue_time.strftime(Job.date_format),
            running=job.is_running and "*" or ""
            )
        if job.start_time:
            info['start_time'] = job.start_time.strftime(Job.date_format)
            dt = job.start_time - job.queue_time
            info['waited'] = fmt_time(secs(dt))
        else:
            info['start_time'] = 'not started'
            info['waited'] = '--'
        if job.end_time:
            info['end_time'] = job.end_time.strftime(Job.date_format)
            dt = job.end_time - job.start_time
            info['duration'] = fmt_time(secs(dt))
        else:
            info['end_time'] = 'not completed'
            if job.start_time:
                dt = datetime.datetime.now() - job.start_time
                info['duration'] = fmt_time(secs(dt))
            else:
                info['duration'] = '--'
        if job.process:
            info['pid'] = 'pid %d' % job.process.pid
        else:
            info['pid'] = 'not running'
        s = (("""
Job:      %(jid)s  [prio: %(priority)s, %(pid)s%(running)s]
Cmd:      %(cmd)s
Queued:   %(queue_time)s
Start:    %(start_time)s
End:      %(end_time)s
Waited:   %(waited)s
Duration: %(duration)s

""".strip() % info) + "\n")
        try:
            print(s)
        except IOError:
            return

@sched_locked
def do_kill(sched, p):
    watch = KillWatch(nprocs=(1+sched.cfg['nprocs']//2))
    for job in sched.jobs_iter(p.args):
        watch.kill(job)
    watch.flush()
    sched._update()
    sched.save_jobs()

@sched_locked
def do_requeue(sched, p):
    watch = KillWatch(nprocs=(1+sched.cfg['nprocs']//2))
    jobs = list(sched.jobs_iter(p.args))
    for job in jobs:
        watch.kill(job)
    watch.flush()
    for job in jobs:
        job.reset()
    sched._update()
    sched.save_jobs()

def do_tail(sched, p, base_cmd=['tail', '-f']):
    jobs = list(sched.jobs_iter(p.args))

    files = []
    for job in jobs:
        if os.path.isfile(job.log_file):
            files.append(job.log_file)

    if base_cmd[0] == 'tail':
        for fn in files:
            print(fn)
            f = open(fn, 'r')
            print("    " + f.readline(), end=' ')
            print("    " + f.readline(), end=' ')
            print("    " + f.readline(), end=' ')
            f.close()
        print("="*79)

    if not files:
        print("No files to tail")
        return

    cmd = base_cmd + files
    os.execvp(cmd[0], cmd)

def do_less(sched, p):
    do_tail(sched, p, ['less', '+k', '-r'])

def get_sub_optparse(p, doc=None):
    if doc is None:
        doc = "Usage: conjob [options] %s [...]" % p.cmd
    return optparse.OptionParser(doc)

def get_run_optparse(p, doc=None):
    p2 = get_sub_optparse(p, doc)
    p2.add_option('-p', '--priority', action='store', type="float",
                  dest='priority', default=0,
                  help="Priority for the started process")
    p2.add_option('-n', '--name-prefix', action='store', type=str,
                  dest='prefix', default=None,
                  help="Prefix for the JIDs of the jobs")
    return p2

def do_run(sched, p):
    p2 = get_run_optparse(p)
    (options, args) = p2.parse_args(p.args)

    sched.add_fg_and_save(args, prefix=options.prefix,
                          priority=options.priority)
    # doesn't return

@sched_locked
def do_bg(sched, p):
    p2 = get_run_optparse(p)
    (options, args) = p2.parse_args(p.args)

    sched.add(args, prefix=options.prefix, priority=options.priority)
    sched.save_jobs()

@sched_locked
def do_bg_many(sched, p):
    p2 = get_run_optparse(p)
    (options, args) = p2.parse_args(p.args)

    if len(args) != 0:
        p.error('wrong number of command-line arguments to bg_many')

    for line in sys.stdin:
        line = line.strip()
        if not line:
            continue
        cmd = shell_split(line)
        sched.add(cmd, prefix=options.prefix, priority=options.priority)
    sched.save_jobs()

@sched_locked
def do_cleanup(sched, p):
    if not p.args:
        p.args = ['all']
    to_remove = [x for x in sched.jobs_iter(p.args) if x.completed]
    for job in to_remove:
        sched.remove(job)
    sched.save_jobs()

@sched_locked
def do_dump_jobs(sched, p):
    if len(p.args) == 0:
        sched.dump_jobs_cfg(sys.stdout)
    elif len(p.args) == 1:
        sched.dump_jobs_cfg(p.args[0])
    else:
        p.error('invalid number of arguments')

@sched_locked
def do_load_jobs(sched, p):
    if len(p.args) == 0:
        sched.load_jobs_cfg(sys.stdout)
    elif len(p.args) == 1:
        sched.load_jobs_cfg(p.args[0])
    else:
        p.error('invalid number of arguments')
    sched.save_jobs()

def do_stats(sched, p):
    import numpy as np
    if not p.args:
        p.args = ['all', '/all']
    jobs = list(sched.jobs_iter(p.args))

    queue_time = []
    run_time = []
    failed_run_time = []
    total = len(jobs)

    for job in jobs:
        if job.started:
            queue_time.append(secs(job.start_time - job.queue_time))
        else:
            queue_time.append(secs(datetime.datetime.now() - job.queue_time))

        if job.completed:
            rt = secs(job.end_time - job.start_time)
            if rt > 0:
                # skip bogus run times
                if job.failed:
                    failed_run_time.append(rt)
                else:
                    run_time.append(rt)

    def xval(v):
        if not v: return [np.nan]
        else: return v

    nremaining = len(queue_time) - len(run_time) - len(failed_run_time)
    est_remaining = nremaining * np.mean(xval(run_time))

    if np.isfinite(est_remaining):
        est_time = datetime.datetime.now() + datetime.timedelta(seconds=est_remaining)
    else:
        est_time = datetime.datetime.now()

    q_hist = ascii_hist(queue_time, bins=8, fmt_label=fmt_time_short)
    r_hist = ascii_hist(run_time, bins=8, fmt_label=fmt_time_short)
    fr_hist = ascii_hist(failed_run_time, bins=8, fmt_label=fmt_time_short)

    def indent(s, n):
        if isinstance(n, int):
            n = " "*n
        s = n + s.replace("\n", "\n" + n)
        if s.endswith(n):
            s = s[:-len(n)]
        return s

    msg = """
Statistics
----------

Jobs: %(jobspec)s

Queue time               [%(cnt_q)d]
  - mean:    %(avg_q)15s
  - median:  %(med_q)15s
  - spread:  %(min_q)15s --- %(max_q)s

%(q_hist)s

Run time                 [%(cnt_r)d]
  - mean:    %(avg_r)15s
  - median:  %(med_r)15s
  - spread:  %(min_r)15s --- %(max_r)s

%(r_hist)s

Run time (failed runs)   [%(cnt_fr)d]
  - mean:    %(avg_fr)15s
  - median:  %(med_fr)15s
  - spread:  %(min_fr)15s --- %(max_fr)s

%(fr_hist)s

Estimated completion time (based on selected completed jobs)
  - remain:  %(est_remaining)s
  - at:      %(est_time)s
""" % dict(jobspec=shell_join(p.args),
           avg_q=fmt_time(np.mean(xval(queue_time))),
           med_q=fmt_time(np.median(xval(queue_time))),
           min_q=fmt_time(np.min(xval(queue_time))),
           max_q=fmt_time(np.max(xval(queue_time))),
           avg_r=fmt_time(np.mean(xval(run_time))),
           med_r=fmt_time(np.median(xval(run_time))),
           min_r=fmt_time(np.min(xval(run_time))),
           max_r=fmt_time(np.max(xval(run_time))),
           avg_fr=fmt_time(np.mean(xval(failed_run_time))),
           med_fr=fmt_time(np.median(xval(failed_run_time))),
           min_fr=fmt_time(np.min(xval(failed_run_time))),
           max_fr=fmt_time(np.max(xval(failed_run_time))),
           cnt_q=len(queue_time),
           cnt_r=len(run_time),
           cnt_fr=len(failed_run_time),
           est_remaining=fmt_time(est_remaining),
           est_time=est_time.strftime('%Y-%m-%d %H:%M %a'),
           q_hist=indent(q_hist, 2),
           r_hist=indent(r_hist, 2),
           fr_hist=indent(fr_hist, 2),
           )
    print(msg)

def secs(dt):
    return 60*60*24*dt.days + dt.seconds + 1e-6*dt.microseconds

def fmt_time(dt):
    if dt != dt:
        return "--"
    elif dt < 60:
        return "%.1f sec" % dt
    elif dt < 60*60:
        mins, secs = divmod(dt, 60)
        return "%d min %d sec" % (mins, secs)
    elif dt < 60*60*24:
        hours, secs = divmod(dt, 60*60)
        mins = round(secs/60.)
        return "%d h %d min" % (hours, mins)
    else:
        days, secs = divmod(dt, 60*60*24)
        hours = round(secs/3600.)
        return "%d days %d hours" % (days, hours)

def fmt_time_short(dt):
    if dt != dt:
        return "--"
    elif dt < 60:
        return "%.1f sec" % dt
    elif dt < 60*60:
        mins, secs = divmod(dt, 60)
        return "%d min" % mins
    elif dt < 60*60*24:
        hours, secs = divmod(dt, 60*60)
        return "%d h" % hours
    else:
        days, secs = divmod(dt, 60*60*24)
        return "%d days" % days


def main(argv):
    p = optparse.OptionParser(__doc__.strip())
    p.allow_interspersed_args = False
    (options, args) = p.parse_args(argv)

    if len(args) < 1:
        p.error("No command given")

    sched = PidScheduler(CFGFILE)

    cmds = dict(schedule=do_schedule,
                setprio=do_setprio,
                list=do_list,
                ls=do_list,
                run=do_run,
                bg=do_bg,
                bg_many=do_bg_many,
                tail=do_tail,
                less=do_less,
                kill=do_kill,
                cleanup=do_cleanup,
                requeue=do_requeue,
                dump_jobs=do_dump_jobs,
                load_jobs=do_load_jobs,
                status=do_stats,
                stats=do_stats,
                stat=do_stats,
                info=do_info)

    cmd = args.pop(0)
    func = cmds.get(cmd)

    p.cmd = cmd
    p.args = args
    p.options = options
    if func is None:
        p.error("Unknown command %s" % cmd)
    else:
        func(sched, p)

#------------------------------------------------------------------------------
# Controlling processes
#------------------------------------------------------------------------------

class Process(object):
    def __init__(self, pid, pgid=None):
        self.pid = pid
        self.pgid = pgid

    def kill(self, sig=None):
        if getattr(self, 'pgid', None) is not None:
            if sig is None:
                os.killpg(self.pgid, signal.SIGINT)
                os.killpg(self.pgid, signal.SIGTERM)
            else:
                os.killpg(self.pgid, sig)
        else:
            if sig is None:
                os.kill(self.pid, signal.SIGINT)
                os.kill(self.pid, signal.SIGTERM)
            else:
                os.kill(self.pid, sig)

    def suspend(self):
        self.kill(signal.SIGSTOP)

    def resume(self):
        self.kill(signal.SIGCONT)

    @property
    def mem_used(self):
        dr = '/proc/%d' % self.pid
        if os.path.isdir(dr):
            f = open('%s/status' % dr, 'r')
            try:
                lines = f.read().split("\n")
                for line in lines:
                    m = re.match(r'^VmRSS:\s*(\d+) kB$', line)
                    if m:
                        return float(m.group(1))
            finally:
                f.close()
        return 0

    @property
    def vm_peak(self):
        dr = '/proc/%d' % self.pid
        if os.path.isdir(dr):
            f = open('%s/status' % dr, 'r')
            try:
                lines = f.read().split("\n")
                for line in lines:
                    m = re.match(r'^VmPeak:\s*(\d+) kB$', line)
                    if m:
                        return float(m.group(1))
            finally:
                f.close()
        return 0

    @property
    def is_running(self):
        dr = '/proc/%d' % self.pid
        if os.path.isdir(dr):
            f = open('%s/stat' % dr, 'r')
            try:
                x = f.read().split()
                return (x[2] != 'T')
            finally:
                f.close()
        else:
            return False

    @property
    def is_alive(self):
        return os.path.isdir('/proc/%d' % self.pid)

    def set_ioprio(self, ioprio=0):
        """Set IO priority"""
        if self.is_alive:
            subprocess.call(['ionice', '-p', str(self.pid), '-n', str(ioprio)],
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    def set_cpu_affinity(self, cpus=[0]):
        if self.is_alive:
            os.sched_setaffinity(self.pid, list(cpus))

    @staticmethod
    def get_loadavg():
        """1 minute load average"""
        f = open('/proc/loadavg', 'r')
        try:
            data = f.read()
            return float(data.split()[0])
        finally:
            f.close()

    @staticmethod
    def get_num_running():
        """Number of currently running processes (likely including this one)"""
        f = open('/proc/stat', 'r')
        try:
            for line in f:
                if line.startswith('procs_running '):
                    return int(line[14:])
            raise RuntimeError("Couldn't get the number of running processes")
        finally:
            f.close()

    @staticmethod
    def get_ncpus():
        f = open('/proc/cpuinfo', 'r')
        try:
            ncpus = 0
            for line in f:
                if line.startswith('processor'):
                    ncpus += 1
            return ncpus
        finally:
            f.close()

    @staticmethod
    def get_total_mem():
        return Process._meminfo().get('MemTotal', 1e99)

    @staticmethod
    def get_free_mem():
        info = Process._meminfo()
        return (info.get('MemFree', 1e99) + info.get('Cached', 0)
                + info.get('Buffers', 0))

    @staticmethod
    def get_total_swap():
        return Process._meminfo().get('SwapTotal', 1e99)

    @staticmethod
    def get_free_swap():
        return Process._meminfo().get('SwapFree', 1e99)

    @staticmethod
    def _meminfo():
        f = open('/proc/meminfo', 'r')
        try:
            lines = f.read().split("\n")
            info = {}
            for line in lines:
                m = re.match(r'^([a-zA-Z]+):\s*(\d+) kB$', line)
                if m:
                    info[m.group(1)] = float(m.group(2))
            return info
        finally:
            f.close()

    @classmethod
    def fork_to_background(cls, cmd, logfile, nice=None, ioprio=None,
                           cwd=None, msg=""):
        """
        Fork command to background.

        Creates two subprocesses::

           watcher (daemonized)
           \- cmd  (new process group leader)

        The watcher process is responsible for saving the command return code
        status. PID+PGID of cmd are used for job control in later calls to
        conjob schedule.

        """
        (pid_pipe_r, pid_pipe_w) = os.pipe()

        # 1st fork
        pid = os.fork()
        if pid > 0:
            # parent
            pid = int(os.read(pid_pipe_r, 64))
            os.close(pid_pipe_r)
            os.close(pid_pipe_w)
            return cls(pid, pgid=pid)

        # invoke in background
        os.setsid()

        # 2nd fork to remove session leader status
        pid = os.fork()
        if pid > 0:
            # dummy parent
            sys.exit(0)

        # redirect output + close input
        sys.stdout.flush()
        sys.stderr.flush()
        if isinstance(logfile, str):
            f = open(logfile, 'ab')
            failfile = logfile + '.fail'
        else:
            f = logfile
            failfile = None
        f2 = open(os.devnull, 'rb')
        os.dup2(f2.fileno(), sys.stdin.fileno())
        os.dup2(f.fileno(), sys.stdout.fileno())
        os.dup2(f.fileno(), sys.stderr.fileno())

        # run
        cls.exec_cmd(cmd, nice=nice, ioprio=ioprio, stop=False, msg=msg,
                     cwd=cwd, failfile=failfile, pid_pipe=pid_pipe_w,
                     setpgid=True)

    @classmethod
    def exec_cmd(cls, cmd, nice=None, ioprio=None, stop=False, cwd=None,
                 msg="", failfile=None, pid_pipe=None, setpgid=False):
        self = cls(os.getpid())

        if stop:
            self.suspend()

        if msg:
            print(msg)
        print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),os.getcwd())
        print(shell_join(cmd))
        print("-"*79)
        sys.stdout.flush()
        sys.stderr.flush()
        if failfile and os.path.isfile(failfile):
            os.unlink(failfile)

        try:
            # prepare environment
            if nice is not None:
                os.nice(nice)
            if ioprio is not None:
                self.set_ioprio(ioprio)
            if cwd:
                os.chdir(cwd)
            # run
            if setpgid:
                proc = subprocess.Popen(cmd, preexec_fn=lambda: os.setpgid(0, 0))
                pid = proc.pid
            else:
                pid = os.spawnvp(os.P_NOWAIT, cmd[0], cmd)
            ret = 0
        except:
            # spawning failed!
            pid = os.getpid()
            ret = -1

        # deliver pid info to the parent
        if pid_pipe is not None:
            os.write(pid_pipe, b'%d\n' % pid)
            os.close(pid_pipe)

        # capture return code
        while True and ret == 0:
            try:
                pid, ret = os.waitpid(pid, 0)
                break
            except KeyboardInterrupt:
                pass

        if ret != 0 and failfile:
            f = open(failfile, 'w')
            f.write('%d\n' % ret)
            f.close()
        sys.exit(0)

    def __repr__(self):
        return "<Process %d (pgid %r)>" % (self.pid, getattr(self, 'pgid', None))

class KillWatch(object):
    def __init__(self, nprocs):
        self.queue = []
        self.nprocs = nprocs

    def mass_kill(self, processes):
        for process in processes:
            self.kill(process)

    def kill(self, process):
        process.kill()
        if not process.is_alive:
            return

        self.queue.append(process)
        process.resume()

        time.sleep(0.05)
        for process in list(self.queue):
            if not process.is_alive:
                self.queue.remove(process)

        if len(self.queue) > self.nprocs:
            self.flush()

    def flush(self):
        for k in range(100):
            alive = False
            for process in self.queue:
                if process.is_alive:
                    alive = True
                    break
            if not alive:
                break
            time.sleep(0.05)
        for process in self.queue:
            if process.is_alive:
                process.kill(signal.SIGKILL)
                process.kill(signal.SIGKILL)
        del self.queue[:]

#------------------------------------------------------------------------------
# Helper routines
#------------------------------------------------------------------------------

def shell_split(s):
    parts = []
    buf = ""
    last_quote = None
    escape = False

    for c in s:
        if escape:
            buf += c
            escape = False
        elif c in ("'", "\""):
            if last_quote == c:
                last_quote = None
            elif last_quote is None:
                last_quote = c
            else:
                buf += c
        elif c == "\\":
            escape = True
        elif c.isspace() and last_quote is None:
            if buf:
                parts.append(buf)
            buf = ""
        else:
            buf += c
    if buf:
        parts.append(buf)

    return parts

def shell_join(parts):
    items = []
    for p in parts:
        if ' ' in p or '\t' in p:
            if '=' in p:
                a, b = p.split('=', 1)
                if not (' ' in a or '\t' in a):
                    items.append("%s=\"%s\"" % (a.replace('"', '\\"'),
                                                b.replace('"', '\\"')))
                    continue
            items.append("\"%s\"" % p.replace('"', '\\"'))
        else:
            items.append(p)
    return " ".join(items)

def ascii_hist(x, fmt_label=None, bins=6, logbins=True):
    import numpy as np

    x = np.asarray(x)

    if x.size == 0:
        return ""

    if logbins and isinstance(bins, int):
        bins = np.logspace(np.log10(max(x.min(),1e-14)), np.log10(x.max()),
                           bins)
        if x.min() == 0:
            bins[0] = 0

    hist, edges = np.histogram(x, bins=bins)

    ncols = 40
    rows = []

    if fmt_label is None:
        fmt_label = lambda x: "%g" % x
        
    if hist.max() == 0:
        return ""

    scale = ncols / float(hist.max())

    for j, bin in enumerate(hist):
        i = int(round(scale*bin))
        if i == 0: continue
        s = "*" * i + " " * (ncols-i) + "    "
        label = "%s .. %s" % (fmt_label(edges[j]), fmt_label(edges[j+1]))
        rows.append((s, label))

    nlabel = max(len(r[1]) for r in rows)
    nlabel = max(nlabel, 70-ncols)
    label_fmt = "%%%ds  " % nlabel

    rows = [(label_fmt%r[1]) + r[0] for r in rows]

    return "\n".join(rows)

if __name__ == "__main__":
    main(sys.argv[1:])
